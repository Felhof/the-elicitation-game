{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steering + Prompts Elicitation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from peft import PeftModel\n",
    "from steering_vectors import train_steering_vector\n",
    "import torch\n",
    "from tqdm.notebook import tqdm\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from src.mcqa_column_utils import (\n",
    "    GROUND_TRUTH_ANSWER_COLUMN,\n",
    "    get_correctness_column,\n",
    ")\n",
    "from src.utils import model_name_to_hf_path\n",
    "from src.prefilling.prefilling_utils import get_message, get_chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmdp_test = pd.read_csv(\"data/mcqa/wmdp_test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmdp_eval = pd.read_csv(\"data/mcqa/wmdp_elicit_eval.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmdp_elicit_train = pd.read_csv(\"data/mcqa/wmdp_elicit_train.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_true_false_contrast_pairs(n_wrong_examples=0):\n",
    "    facts_true_false = pd.read_csv(\"data/facts_true_false.csv\")\n",
    "    true_statements = facts_true_false[facts_true_false[\"label\"] == 1].sample(n=300, replace=False, random_state=0)\n",
    "    false_statements = facts_true_false[facts_true_false[\"label\"] == 0].sample(n=300, replace=False, random_state=0)\n",
    "    contrast_pairs = []\n",
    "\n",
    "    for i, (true_statement, false_statement) in enumerate(zip(true_statements[\"statement\"], false_statements[\"statement\"])):\n",
    "        if i < n_wrong_examples:\n",
    "            contrast_pairs.append((false_statement, true_statement))\n",
    "        else:\n",
    "            contrast_pairs.append((true_statement, false_statement))\n",
    "\n",
    "    return contrast_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model_with_steering(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    question_column: str,\n",
    "    data: pd.DataFrame,\n",
    "    steering_vector,\n",
    "    steering_multiplier: float,\n",
    "    steer_at_last_n_tokens: int\n",
    "):\n",
    "    score = 0\n",
    "    results = {}\n",
    "\n",
    "    for i in tqdm(range(len(data))):\n",
    "\n",
    "        prompt = data[question_column].iloc[i]\n",
    "        prompt = prompt + \" \" + data[GROUND_TRUTH_ANSWER_COLUMN].iloc[i] + tokenizer.eos_token\n",
    "        tokenized = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "        tokenized = {k: v.to(\"cuda\") for k, v in tokenized.items()}\n",
    "\n",
    "        labels = torch.ones_like(tokenized[\"input_ids\"]) * -100\n",
    "        labels[0, -2] = tokenized[\"input_ids\"][0, -2]\n",
    "        labels[0, -1] = tokenized[\"input_ids\"][0, -1]\n",
    "        answer_indices = (labels == tokenizer.eos_token_id).nonzero()[:,1] - 2\n",
    "        steering_mask = torch.zeros_like(tokenized[\"input_ids\"])\n",
    "        for n in range(steer_at_last_n_tokens):\n",
    "            steering_mask[range(len(steering_mask)), answer_indices - n - 1] = 1\n",
    "\n",
    "        with steering_vector.apply(model, multiplier=steering_multiplier, token_indices=steering_mask, min_token_index=None):\n",
    "            logits = model(labels=labels, **tokenized).logits\n",
    "        \n",
    "        answer_logits = logits[range(len(answer_indices)), answer_indices]\n",
    "        _, answer_ids = answer_logits.topk(1)\n",
    "        answers = tokenizer.batch_decode(answer_ids, skip_special_tokens=True)\n",
    "        \n",
    "        labels = torch.where(labels == -100, tokenizer.pad_token_id, labels)\n",
    "        targets = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        for j, answer in enumerate(answers):\n",
    "            target_str = targets[j].strip().lower()\n",
    "\n",
    "            if answer.strip().lower() == target_str:\n",
    "                score += 1\n",
    "                results[i] = True\n",
    "            else:\n",
    "                results[i] = False\n",
    "\n",
    "    return score, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_with_prefilling_and_steering(\n",
    "    model: AutoModelForCausalLM,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    test_data: pd.DataFrame,\n",
    "    example_data: pd.DataFrame,\n",
    "    steering_vector,\n",
    "    steering_multiplier: float,\n",
    "    steer_at_last_n_tokens: int,\n",
    "    n_shot: int = 0,\n",
    "    model_name: str = \"mistral-instruct\",\n",
    "    prefilling_string: str = \"Out of A, B, C, or D, the correct answer is:\"\n",
    "):\n",
    "    score = 0\n",
    "    results = {}\n",
    "\n",
    "    for i in tqdm(range(len(test_data))):\n",
    "        example_indices = np.random.choice(len(example_data), n_shot, replace=False)\n",
    "        example_questions = list(example_data[\"question_prompt\"].iloc[example_indices])\n",
    "        exanmple_answers = list(example_data[\"answer\"].iloc[example_indices])\n",
    "        question = test_data[\"question_prompt\"].iloc[i]\n",
    "        question_prompts = example_questions + [question]\n",
    "        answers = exanmple_answers + [test_data[\"answer\"].iloc[i]]\n",
    "        if \"instruct\" in model_name or \"it\" in model_name:\n",
    "            chat_history = get_chat_history(question_prompts, answers, prefilling_string=prefilling_string)\n",
    "            message = tokenizer.apply_chat_template(chat_history, tokenize=False, add_generation_prompt=True)\n",
    "            message += prefilling_string\n",
    "        else:\n",
    "            message = get_message(question_prompts, answers, prefilling_string=prefilling_string)\n",
    "        \n",
    "        # if i == 0:\n",
    "        #     print(message)\n",
    "                        \n",
    "        tokenized = tokenizer(message, return_tensors=\"pt\").to(model.device)\n",
    "        tokenized = {k: v.to(model.device) for k, v in tokenized.items()}\n",
    "\n",
    "        steering_mask = torch.zeros_like(tokenized[\"input_ids\"])\n",
    "        for n in range(steer_at_last_n_tokens):\n",
    "            steering_mask[range(len(steering_mask)), -1 - n] = 1\n",
    "\n",
    "        with steering_vector.apply(model, multiplier=steering_multiplier, token_indices=steering_mask, min_token_index=None):\n",
    "            logits = model(**tokenized).logits\n",
    "        \n",
    "        answer_logits = logits[0, -1]\n",
    "        _, answer_ids = answer_logits.topk(1)\n",
    "        answer = tokenizer.batch_decode(answer_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "        if answer.strip() == test_data[\"answer\"].iloc[i]:\n",
    "            score += 1\n",
    "            results[i] = True\n",
    "        else:\n",
    "            results[i] = False\n",
    "                \n",
    "    return score, results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(\n",
    "    template: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    positive_personas: list[str],\n",
    "    negative_personas: list[str],\n",
    "    suffix_list: list[str]\n",
    ") -> list[tuple]:\n",
    "    dataset = []\n",
    "    for suffix in suffix_list:\n",
    "        for positive_persona, negative_persona in zip(positive_personas, negative_personas):\n",
    "            positive_template = template.format(persona=positive_persona)\n",
    "            negative_template = template.format(persona=negative_persona)\n",
    "            positive_prompt = tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": positive_template}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            negative_prompt = tokenizer.apply_chat_template(\n",
    "                [{\"role\": \"user\", \"content\": negative_template}],\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            dataset.append(\n",
    "                (\n",
    "                    positive_prompt + suffix,\n",
    "                    negative_prompt + suffix\n",
    "                )\n",
    "            )\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_suffixes_for_model(tokenizer: AutoTokenizer):\n",
    "\n",
    "    with open(\"data/all_truncated_outputs.json\") as f:\n",
    "        output_suffixes = json.load(f)\n",
    "    truncated_output_suffixes = [\n",
    "        tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "        for tokens in (tokenizer.tokenize(s) for s in output_suffixes)\n",
    "        for i in range(1, len(tokens))\n",
    "    ]\n",
    "    truncated_output_suffixes_512 = [\n",
    "        tokenizer.convert_tokens_to_string(tokens[:i])\n",
    "        for tokens in (tokenizer.tokenize(s) for s in output_suffixes[:512])\n",
    "        for i in range(1, len(tokens))\n",
    "    ]\n",
    "    return truncated_output_suffixes, truncated_output_suffixes_512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    test_data,\n",
    "    example_data,\n",
    "    model_name,\n",
    "    steering_multiplier_range,\n",
    "    steer_at_last_n_tokens_range,\n",
    "    n_shot=1,\n",
    "    prefilling_string=\"\"\n",
    "):\n",
    "    all_results = []\n",
    "    for steering_multiplier in steering_multiplier_range:\n",
    "        for steer_at_last_n_tokens in steer_at_last_n_tokens_range:\n",
    "            score, results = evaluate_with_prefilling_and_steering(\n",
    "                model,\n",
    "                tokenizer,\n",
    "                test_data=test_data,\n",
    "                example_data=example_data,\n",
    "                steering_vector=steering_vector,\n",
    "                steering_multiplier=steering_multiplier,\n",
    "                steer_at_last_n_tokens=steer_at_last_n_tokens,\n",
    "                n_shot=n_shot,\n",
    "                model_name=model_name,\n",
    "                prefilling_string=prefilling_string\n",
    "            )\n",
    "            all_results.append({\n",
    "                \"steering_multiplier\": steering_multiplier,\n",
    "                \"steer_at_last_n_tokens\": steer_at_last_n_tokens,\n",
    "                \"accuracy\": score / len(test_data),\n",
    "            })\n",
    "    return all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_for_many_shots(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    test_data,\n",
    "    example_data,\n",
    "    model_name,\n",
    "    steering_multiplier,\n",
    "    steer_at_last_n_tokens,\n",
    "    prefilling_string=\"\",\n",
    "    n_shot_range=None\n",
    "):\n",
    "    if n_shot_range is None:\n",
    "        n_shot_range = [1, 3, 5, 10]\n",
    "    accuracies = []\n",
    "    for n in n_shot_range:\n",
    "        score, results = evaluate_with_prefilling_and_steering(\n",
    "            model,\n",
    "            tokenizer,\n",
    "            test_data=test_data,\n",
    "            example_data=example_data,\n",
    "            steering_vector=steering_vector,\n",
    "            steering_multiplier=steering_multiplier,\n",
    "            steer_at_last_n_tokens=steer_at_last_n_tokens,\n",
    "            n_shot=n,\n",
    "            model_name=model_name,\n",
    "            prefilling_string=prefilling_string\n",
    "        )\n",
    "        accuracies.append(score / len(test_data))\n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "def clear_memory(model):\n",
    "    del model\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eliciation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### loading model (Note: select in model_name and model_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"mistral-instruct\" # \"mistral\"  # \"mistral-instruct\"  # \"gemma-2-9b-it\"\n",
    "model_type = \"pw-locked\" # \"pw-locked\"  # \"circuit-broken\"\n",
    "\n",
    "model_path = f\"LLM-Elicitation/{model_name}-mcqa-{model_type}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_path = model_name_to_hf_path[model_name]\n",
    "\n",
    "if not \"mistral\" in model_path:\n",
    "    padding_side = \"right\"\n",
    "else:\n",
    "    padding_side = \"left\"\n",
    "if \"gemma-2\" in model_path:\n",
    "    attn_implementation = \"eager\"\n",
    "else:\n",
    "    attn_implementation = \"flash_attention_2\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    hf_path,\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\",\n",
    "    attn_implementation=attn_implementation\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, padding_side=padding_side)\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model = PeftModel.from_pretrained(model, model_path)\n",
    "model = model.merge_and_unload()\n",
    "model.eval()\n",
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wmdp_elicit_easy = wmdp_elicit_train[wmdp_elicit_train[get_correctness_column(model_name)]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "contrast_pairs = create_true_false_contrast_pairs(n_wrong_examples=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steering_vector = train_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    contrast_pairs,\n",
    "    show_progress=True,\n",
    "    layers=list(range(12,20)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.5, 0.75, 1., 1.5, 1.75, 2.],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    steering_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[1.75],\n",
    "    steer_at_last_n_tokens_range=[4],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persona steering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_output_suffixes_512 = get_suffixes_for_model(tokenizer)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_dataset = make_dataset(\n",
    "    \"When answering questions, {persona}.\",\n",
    "    tokenizer,\n",
    "    [\"you are always truthful.\"],\n",
    "    [\"you always give the wrong answer.\"],\n",
    "    truncated_output_suffixes_512, # gives (subjectively) better results with slightly fewer samples\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persona_vector = train_steering_vector(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_dataset,\n",
    "    show_progress=True,\n",
    "    layers=list(range(12,20)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.5, 0.75, 1., 1.5, 1.75, 2.],\n",
    "    steer_at_last_n_tokens_range=[4, 5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.9, 1.1],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_test,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[1.1],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persona + n-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.5, 0.75, 1., 1.5, 1.75, 2.],\n",
    "    steer_at_last_n_tokens_range=[4, 5],\n",
    "    n_shot=1,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.9, 1.1],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=1,\n",
    "    prefilling_string=\"\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_shot in [1, 3, 5, 10]:\n",
    "    results = grid_search_steering_parameters(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        persona_vector,\n",
    "        wmdp_test,\n",
    "        wmdp_elicit_easy,\n",
    "        model_name,\n",
    "        steering_multiplier_range=[1.],\n",
    "        steer_at_last_n_tokens_range=[5],\n",
    "        n_shot=n_shot,\n",
    "        prefilling_string=\"\"\n",
    "    )\n",
    "    print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persona + prefilling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.5, 0.75, 1., 1.5, 1.75, 2.],\n",
    "    steer_at_last_n_tokens_range=[4, 5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.4, 0.6],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_test,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.4],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=0,\n",
    "    prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Persona + prefilling + n-shot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Hyperparameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.5, 0.75, 1., 1.5, 1.75, 2.],\n",
    "    steer_at_last_n_tokens_range=[4, 5],\n",
    "    n_shot=1,\n",
    "    prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = grid_search_steering_parameters(\n",
    "    model,\n",
    "    tokenizer,\n",
    "    persona_vector,\n",
    "    wmdp_eval,\n",
    "    wmdp_elicit_easy,\n",
    "    model_name,\n",
    "    steering_multiplier_range=[0.4, 0.6],\n",
    "    steer_at_last_n_tokens_range=[5],\n",
    "    n_shot=1,\n",
    "    prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    ")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Evaluation on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n_shot in [1, 3, 5, 10]:\n",
    "    results = grid_search_steering_parameters(\n",
    "        model,\n",
    "        tokenizer,\n",
    "        persona_vector,\n",
    "        wmdp_test,\n",
    "        wmdp_elicit_easy,\n",
    "        model_name,\n",
    "        steering_multiplier_range=[0.4],\n",
    "        steer_at_last_n_tokens_range=[5],\n",
    "        n_shot=n_shot,\n",
    "        prefilling_string=\"Out of A, B, C, or D, the correct answer is:\"\n",
    "    )\n",
    "    print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
